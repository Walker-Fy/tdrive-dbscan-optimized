{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "performance-analysis",
   "metadata": {},
   "source": [
    "# DBSCAN聚类算法性能分析\n",
    "\n",
    "本笔记本用于分析串行和并行DBSCAN算法的性能差异，包括执行时间、内存使用和可扩展性分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## 1. 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "id": "import-libraries",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# 导入所需库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体和图表样式\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Python版本:\", sys.version)\n",
    "print(\"NumPy版本:\", np.__version__)\n",
    "print(\"Pandas版本:\", pd.__version__)\n",
    "print(\"CPU核心数:\", mp.cpu_count())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "load-modules",
   "metadata": {},
   "source": [
    "## 2. 导入项目模块"
   ]
  },
  {
   "cell_type": "code",
   "id": "import-project-modules",
   "metadata": {},
   "source": [
    "# 导入项目模块\n",
    "from src.clustering.dbscan_sequential import DBSCANSequential\n",
    "from src.clustering.dbscan_parallel import DBSCANParallel\n",
    "from src.clustering.utils import compute_distance_matrix, build_spatial_index\n",
    "\n",
    "from src.profiling.time_profiler import TimeProfiler, profile_function\n",
    "from src.profiling.memory_profiler import MemoryProfiler, track_memory_usage\n",
    "from src.profiling.performance_analyzer import PerformanceAnalyzer, compare_implementations\n",
    "\n",
    "from src.visualization.plot_performance import PerformanceVisualizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "generate-test-data",
   "metadata": {},
   "source": [
    "## 3. 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "id": "create-test-datasets",
   "metadata": {},
   "source": [
    "def generate_test_datasets():\n",
    "    \"\"\"生成不同大小的测试数据集\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # 数据集大小\n",
    "    sizes = [100, 500, 1000, 2000, 5000, 10000, 20000]\n",
    "    \n",
    "    print(\"生成测试数据集...\")\n",
    "    \n",
    "    for size in sizes:\n",
    "        # 生成聚类数据（模拟热点区域）\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # 创建3个聚类中心\n",
    "        cluster_centers = np.array([\n",
    "            [39.9, 116.3],  # 市中心\n",
    "            [40.0, 116.5],  # 北部区域\n",
    "            [39.8, 116.4]   # 南部区域\n",
    "        ])\n",
    "        \n",
    "        points = []\n",
    "        \n",
    "        # 为每个聚类生成点\n",
    "        cluster_sizes = np.random.dirichlet([1, 1, 1]) * size\n",
    "        cluster_sizes = cluster_sizes.astype(int)\n",
    "        cluster_sizes[-1] = size - sum(cluster_sizes[:-1])  # 确保总数正确\n",
    "        \n",
    "        for i, center in enumerate(cluster_centers):\n",
    "            n_points = cluster_sizes[i]\n",
    "            if n_points > 0:\n",
    "                cluster_points = np.random.randn(n_points, 2) * 0.01 + center\n",
    "                points.append(cluster_points)\n",
    "        \n",
    "        # 添加一些噪声点\n",
    "        n_noise = int(size * 0.1)  # 10%的噪声点\n",
    "        noise_points = np.random.uniform(\n",
    "            low=[39.7, 116.2],\n",
    "            high=[40.1, 116.7],\n",
    "            size=(n_noise, 2)\n",
    "        )\n",
    "        points.append(noise_points)\n",
    "        \n",
    "        # 合并所有点\n",
    "        all_points = np.vstack(points)\n",
    "        \n",
    "        # 随机打乱\n",
    "        np.random.shuffle(all_points)\n",
    "        \n",
    "        datasets[f\"dataset_{size}\"] = {\n",
    "            \"points\": all_points,\n",
    "            \"size\": len(all_points),\n",
    "            \"n_clusters\": 3,\n",
    "            \"n_noise\": n_noise\n",
    "        }\n",
    "        \n",
    "        print(f\"  数据集 {size}: {len(all_points)} 个点 (3个聚类 + {n_noise}个噪声点)\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# 生成测试数据\n",
    "test_datasets = generate_test_datasets()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "visualize-test-data",
   "metadata": {},
   "source": [
    "# 可视化测试数据集\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, data) in enumerate(list(test_datasets.items())[:7]):  # 显示前7个数据集\n",
    "    ax = axes[idx]\n",
    "    points = data[\"points\"]\n",
    "    \n",
    "    ax.scatter(points[:, 1], points[:, 0], \n",
    "               s=20, alpha=0.6, edgecolors='none')\n",
    "    \n",
    "    ax.set_xlabel('经度')\n",
    "    ax.set_ylabel('纬度')\n",
    "    ax.set_title(f'{data[\"size\"]} 个点')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 隐藏多余的子图\n",
    "for idx in range(len(test_datasets), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('测试数据集可视化', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sequential-benchmark",
   "metadata": {},
   "source": [
    "## 4. 串行DBSCAN基准测试"
   ]
  },
  {
   "cell_type": "code",
   "id": "run-sequential-benchmark",
   "metadata": {},
   "source": [
    "def benchmark_sequential_dbscan(datasets, eps=0.01, min_samples=5):\n",
    "    \"\"\"运行串行DBSCAN基准测试\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"串行DBSCAN基准测试\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, data in datasets.items():\n",
    "        points = data[\"points\"]\n",
    "        size = data[\"size\"]\n",
    "        \n",
    "        print(f\"\\n测试数据集: {name} ({size} 个点)\")\n",
    "        \n",
    "        # 创建DBSCAN聚类器\n",
    "        dbscan = DBSCANSequential(\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        \n",
    "        # 使用性能分析器\n",
    "        time_profiler = TimeProfiler(enable_profiling=False)\n",
    "        memory_profiler = MemoryProfiler(track_detailed=False)\n",
    "        \n",
    "        # 开始内存分析\n",
    "        memory_profiler.start()\n",
    "        memory_before = memory_profiler.take_snapshot(\"开始前\")\n",
    "        \n",
    "        # 执行聚类\n",
    "        start_time = time.time()\n",
    "        dbscan.fit(points)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # 内存分析\n",
    "        memory_after = memory_profiler.take_snapshot(\"结束后\")\n",
    "        memory_usage = memory_after.memory_usage_mb - memory_before.memory_usage_mb\n",
    "        \n",
    "        # 停止内存分析\n",
    "        memory_profiler.stop()\n",
    "        \n",
    "        # 获取聚类结果\n",
    "        stats = dbscan.get_cluster_stats()\n",
    "        \n",
    "        print(f\"  执行时间: {execution_time:.4f} 秒\")\n",
    "        print(f\"  内存使用: {memory_usage:.2f} MB\")\n",
    "        print(f\"  聚类数量: {stats['n_clusters']}\")\n",
    "        print(f\"  噪声点数量: {stats['n_noise']}\")\n",
    "        \n",
    "        # 保存结果\n",
    "        results[name] = {\n",
    "            \"size\": size,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"memory_usage_mb\": memory_usage,\n",
    "            \"peak_memory_mb\": memory_after.peak_memory_mb,\n",
    "            \"n_clusters\": stats[\"n_clusters\"],\n",
    "            \"n_noise\": stats[\"n_noise\"],\n",
    "            \"n_core_points\": stats[\"n_core_points\"]\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行串行基准测试\n",
    "sequential_results = benchmark_sequential_dbscan(test_datasets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "analyze-sequential-scalability",
   "metadata": {},
   "source": [
    "# 分析串行版本的可扩展性\n",
    "if sequential_results:\n",
    "    # 创建DataFrame\n",
    "    seq_df = pd.DataFrame.from_dict(sequential_results, orient='index')\n",
    "    seq_df = seq_df.sort_values('size')\n",
    "    \n",
    "    print(\"串行DBSCAN性能分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    display(seq_df)\n",
    "    \n",
    "    # 可视化可扩展性\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. 执行时间 vs 数据大小\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(seq_df['size'], seq_df['execution_time'], 'bo-', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('数据点数量')\n",
    "    ax1.set_ylabel('执行时间 (秒)')\n",
    "    ax1.set_title('执行时间 vs 数据大小')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # 2. 内存使用 vs 数据大小\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(seq_df['size'], seq_df['memory_usage_mb'], 'ro-', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('数据点数量')\n",
    "    ax2.set_ylabel('内存使用 (MB)')\n",
    "    ax2.set_title('内存使用 vs 数据大小')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # 3. 时间复杂度分析\n",
    "    ax3 = axes[2]\n",
    "    sizes = seq_df['size'].values\n",
    "    times = seq_df['execution_time'].values\n",
    "    \n",
    "    # 计算不同复杂度模型的拟合\n",
    "    # O(n) 参考线\n",
    "    n_fit = times[0] * sizes / sizes[0]\n",
    "    # O(n²) 参考线\n",
    "    n2_fit = times[0] * (sizes / sizes[0]) ** 2\n",
    "    \n",
    "    ax3.loglog(sizes, times, 'ko-', linewidth=2, markersize=8, label='实际时间')\n",
    "    ax3.loglog(sizes, n_fit, 'r--', alpha=0.7, label='O(n)')\n",
    "    ax3.loglog(sizes, n2_fit, 'g--', alpha=0.7, label='O(n²)')\n",
    "    \n",
    "    ax3.set_xlabel('数据点数量 (log)')\n",
    "    ax3.set_ylabel('执行时间 (log)')\n",
    "    ax3.set_title('时间复杂度分析')\n",
    "    ax3.grid(True, alpha=0.3, which='both')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.suptitle('串行DBSCAN可扩展性分析', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 计算实际复杂度\n",
    "    print(\"\\n时间复杂度估计:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 使用最后两个点估计增长率\n",
    "    if len(sizes) >= 2:\n",
    "        n1, n2 = sizes[-2], sizes[-1]\n",
    "        t1, t2 = times[-2], times[-1]\n",
    "        \n",
    "        # 估计复杂度指数\n",
    "        exponent = np.log(t2/t1) / np.log(n2/n1)\n",
    "        print(f\"复杂度指数: {exponent:.3f}\")\n",
    "        \n",
    "        if exponent < 1.2:\n",
    "            print(\"推断复杂度: 接近 O(n)\")\n",
    "        elif exponent < 1.8:\n",
    "            print(f\"推断复杂度: O(n^{exponent:.2f})，接近 O(n log n)\")\n",
    "        else:\n",
    "            print(f\"推断复杂度: O(n^{exponent:.2f})，接近 O(n²)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "parallel-benchmark",
   "metadata": {},
   "source": [
    "## 5. 并行DBSCAN性能测试"
   ]
  },
  {
   "cell_type": "code",
   "id": "run-parallel-benchmark",
   "metadata": {},
   "source": [
    "def benchmark_parallel_dbscan(datasets, n_jobs_list=[1, 2, 4, 8], \n",
    "                            eps=0.01, min_samples=5):\n",
    "    \"\"\"运行并行DBSCAN基准测试\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"并行DBSCAN基准测试\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 选择中等大小的数据集进行并行测试\n",
    "    test_dataset_name = \"dataset_5000\"\n",
    "    if test_dataset_name not in datasets:\n",
    "        test_dataset_name = list(datasets.keys())[3]  # 选择第4个数据集\n",
    "    \n",
    "    data = datasets[test_dataset_name]\n",
    "    points = data[\"points\"]\n",
    "    size = data[\"size\"]\n",
    "    \n",
    "    print(f\"测试数据集: {test_dataset_name} ({size} 个点)\")\n",
    "    print(f\"测试不同的工作进程数: {n_jobs_list}\")\n",
    "    \n",
    "    for n_jobs in n_jobs_list:\n",
    "        print(f\"\\n工作进程数: {n_jobs}\")\n",
    "        \n",
    "        # 创建并行DBSCAN聚类器\n",
    "        dbscan = DBSCANParallel(\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean',\n",
    "            n_jobs=n_jobs,\n",
    "            chunk_size=1000\n",
    "        )\n",
    "        \n",
    "        # 使用性能分析器\n",
    "        time_profiler = TimeProfiler(enable_profiling=False)\n",
    "        memory_profiler = MemoryProfiler(track_detailed=False)\n",
    "        \n",
    "        # 开始内存分析\n",
    "        memory_profiler.start()\n",
    "        memory_before = memory_profiler.take_snapshot(\"开始前\")\n",
    "        \n",
    "        # 执行聚类\n",
    "        start_time = time.time()\n",
    "        dbscan.fit(points)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # 内存分析\n",
    "        memory_after = memory_profiler.take_snapshot(\"结束后\")\n",
    "        memory_usage = memory_after.memory_usage_mb - memory_before.memory_usage_mb\n",
    "        \n",
    "        # 停止内存分析\n",
    "        memory_profiler.stop()\n",
    "        \n",
    "        # 获取聚类结果\n",
    "        stats = dbscan.get_cluster_stats()\n",
    "        perf_stats = dbscan.get_performance_stats()\n",
    "        \n",
    "        print(f\"  执行时间: {execution_time:.4f} 秒\")\n",
    "        print(f\"  内存使用: {memory_usage:.2f} MB\")\n",
    "        print(f\"  聚类数量: {stats['n_clusters']}\")\n",
    "        print(f\"  噪声点数量: {stats['n_noise']}\")\n",
    "        \n",
    "        # 保存结果\n",
    "        results[n_jobs] = {\n",
    "            \"n_jobs\": n_jobs,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"memory_usage_mb\": memory_usage,\n",
    "            \"peak_memory_mb\": memory_after.peak_memory_mb,\n",
    "            \"n_clusters\": stats[\"n_clusters\"],\n",
    "            \"n_noise\": stats[\"n_noise\"],\n",
    "            \"n_core_points\": stats[\"n_core_points\"]\n",
    "        }\n",
    "    \n",
    "    return results, test_dataset_name\n",
    "\n",
    "# 运行并行基准测试\n",
    "parallel_results, test_dataset_name = benchmark_parallel_dbscan(test_datasets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "analyze-parallel-efficiency",
   "metadata": {},
   "source": [
    "# 分析并行效率\n",
    "if parallel_results:\n",
    "    # 创建DataFrame\n",
    "    par_df = pd.DataFrame.from_dict(parallel_results, orient='index')\n",
    "    par_df = par_df.sort_values('n_jobs')\n",
    "    \n",
    "    print(\"并行DBSCAN性能分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    display(par_df)\n",
    "    \n",
    "    # 获取串行版本的时间作为基准\n",
    "    if test_dataset_name in sequential_results:\n",
    "        serial_time = sequential_results[test_dataset_name][\"execution_time\"]\n",
    "        print(f\"\\n串行版本执行时间: {serial_time:.4f} 秒\")\n",
    "        \n",
    "        # 计算加速比和并行效率\n",
    "        par_df['speedup'] = serial_time / par_df['execution_time']\n",
    "        par_df['efficiency'] = par_df['speedup'] / par_df['n_jobs']\n",
    "        \n",
    "        print(\"\\n并行性能指标:\")\n",
    "        print(\"-\" * 40)\n",
    "        display(par_df[['n_jobs', 'execution_time', 'speedup', 'efficiency']])\n",
    "        \n",
    "        # 可视化并行性能\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # 1. 执行时间对比\n",
    "        ax1 = axes[0]\n",
    "        x_pos = range(len(par_df))\n",
    "        ax1.bar(x_pos, par_df['execution_time'], \n",
    "               color='skyblue', alpha=0.7)\n",
    "        ax1.axhline(y=serial_time, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'串行时间: {serial_time:.3f}s')\n",
    "        ax1.set_xlabel('工作进程数')\n",
    "        ax1.set_ylabel('执行时间 (秒)')\n",
    "        ax1.set_title('执行时间对比')\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(par_df['n_jobs'])\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 在柱状图上添加数值\n",
    "        for i, v in enumerate(par_df['execution_time']):\n",
    "            ax1.text(i, v * 1.02, f'{v:.3f}', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 2. 加速比\n",
    "        ax2 = axes[1]\n",
    "        bars = ax2.bar(x_pos, par_df['speedup'], \n",
    "                      color='lightgreen', alpha=0.7)\n",
    "        ax2.plot(x_pos, par_df['n_jobs'], 'ro-', \n",
    "                linewidth=2, markersize=8, label='理想加速比')\n",
    "        ax2.set_xlabel('工作进程数')\n",
    "        ax2.set_ylabel('加速比')\n",
    "        ax2.set_title('并行加速比')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(par_df['n_jobs'])\n",
    "        ax2.legend()\n",
    "        \n",
    "        # 在柱状图上添加数值\n",
    "        for i, v in enumerate(par_df['speedup']):\n",
    "            ax2.text(i, v * 1.02, f'{v:.2f}x', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 3. 并行效率\n",
    "        ax3 = axes[2]\n",
    "        bars = ax3.bar(x_pos, par_df['efficiency'], \n",
    "                      color='gold', alpha=0.7)\n",
    "        ax3.axhline(y=1.0, color='red', linestyle='--', \n",
    "                   linewidth=2, label='理想效率 (100%)')\n",
    "        ax3.set_xlabel('工作进程数')\n",
    "        ax3.set_ylabel('并行效率')\n",
    "        ax3.set_title('并行效率')\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(par_df['n_jobs'])\n",
    "        ax3.set_ylim(0, 1.2)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # 在柱状图上添加百分比\n",
    "        for i, v in enumerate(par_df['efficiency']):\n",
    "            ax3.text(i, v * 1.02, f'{v:.1%}', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.suptitle(f'并行DBSCAN性能分析 ({test_dataset_name})', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 分析并行瓶颈\n",
    "        print(\"\\n并行性能分析:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 找出最优配置\n",
    "        best_idx = par_df['speedup'].idxmax()\n",
    "        best_config = par_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"最优配置: {best_config['n_jobs']} 个工作进程\")\n",
    "        print(f\"最大加速比: {best_config['speedup']:.2f}x\")\n",
    "        print(f\"并行效率: {best_config['efficiency']:.1%}\")\n",
    "        \n",
    "        # 计算Amdahl定律预测\n",
    "        # 假设串行部分比例为 p\n",
    "        # 使用实际数据拟合Amdahl定律\n",
    "        def amdahl_speedup(p, n):\n",
    "            return 1 / ((1 - p) + p / n)\n",
    "        \n",
    "        # 使用最小二乘法估计串行比例\n",
    "        from scipy.optimize import curve_fit\n",
    "        try:\n",
    "            n_jobs_array = par_df['n_jobs'].values\n",
    "            speedup_array = par_df['speedup'].values\n",
    "            \n",
    "            # 拟合Amdahl定律\n",
    "            popt, _ = curve_fit(amdahl_speedup, n_jobs_array, speedup_array, \n",
    "                               bounds=(0, 1))\n",
    "            serial_fraction = popt[0]\n",
    "            \n",
    "            print(f\"\\nAmdahl定律分析:\")\n",
    "            print(f\"  估计的串行部分比例: {serial_fraction:.3f}\")\n",
    "            print(f\"  最大理论加速比 (无限核心): {1/serial_fraction:.2f}x\")\n",
    "            \n",
    "            # 绘制Amdahl定律拟合\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            \n",
    "            n_range = np.linspace(1, max(n_jobs_array) * 2, 100)\n",
    "            theoretical_speedup = amdahl_speedup(serial_fraction, n_range)\n",
    "            \n",
    "            ax.plot(n_range, theoretical_speedup, 'r-', \n",
    "                   linewidth=2, label='Amdahl定律拟合')\n",
    "            ax.scatter(n_jobs_array, speedup_array, s=100, \n",
    "                      color='blue', alpha=0.7, label='实际数据')\n",
    "            ax.plot(n_jobs_array, n_jobs_array, 'g--', \n",
    "                   linewidth=2, alpha=0.5, label='理想加速比')\n",
    "            \n",
    "            ax.set_xlabel('工作进程数')\n",
    "            ax.set_ylabel('加速比')\n",
    "            ax.set_title('Amdahl定律分析')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Amdahl定律分析失败: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-comparison",
   "metadata": {},
   "source": [
    "## 6. 综合性能比较"
   ]
  },
  {
   "cell_type": "code",
   "id": "run-comprehensive-comparison",
   "metadata": {},
   "source": [
    "def run_comprehensive_comparison(datasets):\n",
    "    \"\"\"运行综合性能比较\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"综合性能比较\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 选择中等大小的数据集\n",
    "    test_dataset_name = \"dataset_5000\"\n",
    "    if test_dataset_name not in datasets:\n",
    "        test_dataset_name = list(datasets.keys())[3]\n",
    "    \n",
    "    data = datasets[test_dataset_name]\n",
    "    points = data[\"points\"]\n",
    "    \n",
    "    print(f\"使用数据集: {test_dataset_name} ({len(points)} 个点)\")\n",
    "    \n",
    "    # 创建性能分析器\n",
    "    analyzer = PerformanceAnalyzer(output_dir=\"../results\")\n",
    "    \n",
    "    # 定义要比较的实现\n",
    "    implementations = {\n",
    "        'DBSCAN_Sequential': lambda data: DBSCANSequential(\n",
    "            eps=0.01, min_samples=5, metric='euclidean'\n",
    "        ).fit(data),\n",
    "        \n",
    "        'DBSCAN_Parallel_2jobs': lambda data: DBSCANParallel(\n",
    "            eps=0.01, min_samples=5, metric='euclidean', n_jobs=2\n",
    "        ).fit(data),\n",
    "        \n",
    "        'DBSCAN_Parallel_4jobs': lambda data: DBSCANParallel(\n",
    "            eps=0.01, min_samples=5, metric='euclidean', n_jobs=4\n",
    "        ).fit(data),\n",
    "        \n",
    "        'DBSCAN_Parallel_8jobs': lambda data: DBSCANParallel(\n",
    "            eps=0.01, min_samples=5, metric='euclidean', n_jobs=8\n",
    "        ).fit(data),\n",
    "    }\n",
    "    \n",
    "    # 运行比较\n",
    "    print(\"\\n运行性能比较...\")\n",
    "    comparison_results = compare_implementations(\n",
    "        implementations=implementations,\n",
    "        test_data=points,\n",
    "        test_name=\"dbscan_comprehensive\",\n",
    "        output_dir=\"../results\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n比较结果:\")\n",
    "    print(\"-\" * 40)\n",
    "    display(comparison_results)\n",
    "    \n",
    "    return comparison_results, implementations\n",
    "\n",
    "# 运行综合比较\n",
    "comparison_results, implementations = run_comprehensive_comparison(test_datasets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "visualize-comprehensive-comparison",
   "metadata": {},
   "source": [
    "# 可视化综合比较结果\n",
    "if 'comparison_results' in locals() and not comparison_results.empty:\n",
    "    visualizer = PerformanceVisualizer()\n",
    "    \n",
    "    # 准备性能数据\n",
    "    perf_data = {}\n",
    "    for _, row in comparison_results.iterrows():\n",
    "        perf_data[row['implementation']] = {\n",
    "            'execution_time': row['execution_time'],\n",
    "            'memory_usage_mb': row['memory_usage_mb'],\n",
    "            'peak_memory_mb': row['peak_memory_mb'],\n",
    "            'accuracy': row.get('accuracy', 1.0)\n",
    "        }\n",
    "    \n",
    "    # 创建综合比较图\n",
    "    fig = visualizer.plot_execution_time_comparison(\n",
    "        perf_data,\n",
    "        title=\"DBSCAN算法综合性能比较\",\n",
    "        log_scale=True\n",
    "    )\n",
    "    \n",
    "    # 分析性能提升\n",
    "    print(\"\\n性能提升分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'time_speedup' in comparison_results.columns:\n",
    "        best_impl = comparison_results.loc[comparison_results['time_speedup'].idxmax()]\n",
    "        worst_impl = comparison_results.loc[comparison_results['time_speedup'].idxmin()]\n",
    "        \n",
    "        print(f\"最快实现: {best_impl['implementation']}\")\n",
    "        print(f\"  执行时间: {best_impl['execution_time']:.4f} 秒\")\n",
    "        print(f\"  速度提升: {best_impl['time_speedup']:.2f}x (相对于最快基准)\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"最慢实现: {worst_impl['implementation']}\")\n",
    "        print(f\"  执行时间: {worst_impl['execution_time']:.4f} 秒\")\n",
    "        print(f\"  速度提升: {worst_impl['time_speedup']:.2f}x (相对于最快基准)\")\n",
    "        print()\n",
    "        \n",
    "        # 计算相对提升\n",
    "        speedup_ratio = worst_impl['execution_time'] / best_impl['execution_time']\n",
    "        print(f\"最快 vs 最慢: 加速比 {speedup_ratio:.2f}x\")\n",
    "        \n",
    "        # 内存使用比较\n",
    "        best_mem_impl = comparison_results.loc[comparison_results['memory_usage_mb'].idxmin()]\n",
    "        worst_mem_impl = comparison_results.loc[comparison_results['memory_usage_mb'].idxmax()]\n",
    "        \n",
    "        print(f\"\\n内存使用比较:\")\n",
    "        print(f\"  最省内存: {best_mem_impl['implementation']} ({best_mem_impl['memory_usage_mb']:.2f} MB)\")\n",
    "        print(f\"  最耗内存: {worst_mem_impl['implementation']} ({worst_mem_impl['memory_usage_mb']:.2f} MB)\")\n",
    "        print(f\"  内存比: {worst_mem_impl['memory_usage_mb'] / best_mem_impl['memory_usage_mb']:.2f}x\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "optimization-analysis",
   "metadata": {},
   "source": [
    "## 7. 性能瓶颈分析"
   ]
  },
  {
   "cell_type": "code",
   "id": "analyze-performance-bottlenecks",
   "metadata": {},
   "source": [
    "def analyze_performance_bottlenecks():\n",
    "    \"\"\"分析性能瓶颈\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"性能瓶颈分析\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 使用一个较小的数据集进行详细分析\n",
    "    test_points = test_datasets[\"dataset_1000\"][\"points\"]\n",
    "    \n",
    "    # 分析串行版本的瓶颈\n",
    "    print(\"\\n分析串行DBSCAN性能瓶颈:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 使用详细性能分析\n",
    "    dbscan_sequential = DBSCANSequential(eps=0.01, min_samples=5)\n",
    "    \n",
    "    @profile_function(detailed=True)\n",
    "    def run_sequential():\n",
    "        return dbscan_sequential.fit(test_points)\n",
    "    \n",
    "    result, time_analysis = run_sequential()\n",
    "    \n",
    "    print(f\"执行时间: {time_analysis['execution_time']:.4f} 秒\")\n",
    "    \n",
    "    if 'top_functions' in time_analysis:\n",
    "        print(\"\\n最耗时的函数:\")\n",
    "        for i, func_info in enumerate(time_analysis['top_functions'][:5]):\n",
    "            print(f\"  {i+1}. {func_info['function']}: {func_info['cumtime']:.4f} 秒\")\n",
    "    \n",
    "    # 内存瓶颈分析\n",
    "    print(\"\\n内存瓶颈分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    memory_profiler = MemoryProfiler(track_detailed=True)\n",
    "    \n",
    "    with memory_profiler:\n",
    "        memory_before = memory_profiler.take_snapshot(\"开始前\")\n",
    "        dbscan_sequential.fit(test_points)\n",
    "        memory_after = memory_profiler.take_snapshot(\"结束后\")\n",
    "    \n",
    "    memory_usage = memory_after.memory_usage_mb - memory_before.memory_usage_mb\n",
    "    print(f\"内存使用: {memory_usage:.2f} MB\")\n",
    "    print(f\"峰值内存: {memory_after.peak_memory_mb:.2f} MB\")\n",
    "    \n",
    "    # 查找内存瓶颈\n",
    "    bottlenecks = memory_profiler.find_memory_bottlenecks(threshold_mb=1.0)\n",
    "    if bottlenecks:\n",
    "        print(\"\\n内存瓶颈点:\")\n",
    "        for i, bottleneck in enumerate(bottlenecks[:3]):\n",
    "            print(f\"  {i+1}. {bottleneck['consumer']}: {bottleneck['size_mb']:.2f} MB\")\n",
    "    \n",
    "    # 分析内存模式\n",
    "    patterns = memory_profiler.analyze_memory_patterns()\n",
    "    if patterns:\n",
    "        print(\"\\n内存使用模式:\")\n",
    "        print(f\"  总内存增长: {patterns.get('total_memory_growth_mb', 0):.2f} MB\")\n",
    "        print(f\"  内存增长率: {patterns.get('memory_growth_rate_mb_per_sec', 0):.2f} MB/秒\")\n",
    "        print(f\"  内存峰值数: {patterns.get('n_memory_peaks', 0)}\")\n",
    "    \n",
    "    return time_analysis, memory_profiler\n",
    "\n",
    "# 运行瓶颈分析\n",
    "time_analysis, memory_profiler = analyze_performance_bottlenecks()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "optimization-suggestions",
   "metadata": {},
   "source": [
    "# 生成优化建议\n",
    "if 'time_analysis' in locals() and 'memory_profiler' in locals():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"优化建议\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    # 基于时间分析的优化建议\n",
    "    if 'top_functions' in time_analysis:\n",
    "        top_funcs = time_analysis['top_functions']\n",
    "        \n",
    "        for func_info in top_funcs[:3]:\n",
    "            func_name = func_info['function']\n",
    "            time_spent = func_info['cumtime']\n",
    "            time_percent = (time_spent / time_analysis['execution_time']) * 100\n",
    "            \n",
    "            if time_percent > 20:\n",
    "                suggestions.append(f\"函数 '{func_name}' 占用 {time_percent:.1f}% 的执行时间，是主要瓶颈\")\n",
    "                \n",
    "                # 具体优化建议\n",
    "                if 'distance' in func_name.lower():\n",
    "                    suggestions.append(\"  → 使用KDTree/BallTree加速距离计算\")\n",
    "                    suggestions.append(\"  → 考虑使用向量化计算\")\n",
    "                    suggestions.append(\"  → 缓存距离计算结果\")\n",
    "                \n",
    "                if 'neighbor' in func_name.lower():\n",
    "                    suggestions.append(\"  → 使用空间索引加速邻居搜索\")\n",
    "                    suggestions.append(\"  → 预计算距离矩阵\")\n",
    "                    suggestions.append(\"  → 并行化邻居查询\")\n",
    "                \n",
    "                if 'loop' in func_name.lower():\n",
    "                    suggestions.append(\"  → 使用向量化操作代替Python循环\")\n",
    "                    suggestions.append(\"  → 考虑使用Numba进行JIT编译\")\n",
    "    \n",
    "    # 基于内存分析的优化建议\n",
    "    bottlenecks = memory_profiler.find_memory_bottlenecks(threshold_mb=5.0)\n",
    "    if bottlenecks:\n",
    "        suggestions.append(f\"发现 {len(bottlenecks)} 个内存瓶颈点\")\n",
    "        \n",
    "        for bottleneck in bottlenecks[:2]:\n",
    "            suggestions.append(f\"  → {bottleneck['consumer']} 占用 {bottleneck['size_mb']:.2f} MB\")\n",
    "            suggestions.append(\"    - 考虑使用更紧凑的数据类型\")\n",
    "            suggestions.append(\"    - 及时释放不再使用的对象\")\n",
    "            suggestions.append(\"    - 使用内存映射文件处理大数据\")\n",
    "    \n",
    "    # 通用优化建议\n",
    "    suggestions.extend([\n",
    "        \"通用优化建议:\",\n",
    "        \"1. 使用numpy数组代替Python列表存储数值数据\",\n",
    "        \"2. 使用适当的数据类型（如float32代替float64）\",\n",
    "        \"3. 对于大规模数据，采用分批处理策略\",\n",
    "        \"4. 利用多核CPU进行并行计算\",\n",
    "        \"5. 使用空间索引（如KDTree）加速空间查询\",\n",
    "        \"6. 缓存重复的计算结果\",\n",
    "        \"7. 使用生成器处理流式数据，减少内存占用\"\n",
    "    ])\n",
    "    \n",
    "    # 并行优化建议\n",
    "    suggestions.extend([\n",
    "        \"\\n并行优化建议:\",\n",
    "        f\"1. 系统有 {mp.cpu_count()} 个CPU核心，建议使用 {max(1, mp.cpu_count() - 2)} 个工作进程\",\n",
    "        \"2. 使用共享内存减少进程间数据复制\",\n",
    "        \"3. 实现动态负载均衡，避免工作进程空闲\",\n",
    "        \"4. 考虑任务粒度，避免任务过小导致通信开销过大\",\n",
    "        \"5. 使用异步I/O重叠计算和通信\"\n",
    "    ])\n",
    "    \n",
    "    # 打印所有建议\n",
    "    for suggestion in suggestions:\n",
    "        print(suggestion)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-and-recommendations",
   "metadata": {},
   "source": [
    "## 8. 结论与建议"
   ]
  },
  {
   "cell_type": "code",
   "id": "performance-conclusion",
   "metadata": {},
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"性能分析结论\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "conclusions = []\n",
    "\n",
    "# 串行性能结论\n",
    "if 'sequential_results' in locals() and sequential_results:\n",
    "    seq_df = pd.DataFrame.from_dict(sequential_results, orient='index')\n",
    "    avg_time = seq_df['execution_time'].mean()\n",
    "    avg_memory = seq_df['memory_usage_mb'].mean()\n",
    "    \n",
    "    conclusions.append(\"串行DBSCAN性能:\")\n",
    "    conclusions.append(f\"  • 平均执行时间: {avg_time:.4f} 秒\")\n",
    "    conclusions.append(f\"  • 平均内存使用: {avg_memory:.2f} MB\")\n",
    "    conclusions.append(f\"  • 时间复杂度: 接近 O(n²)（基于实际测量）\")\n",
    "\n",
    "# 并行性能结论\n",
    "if 'parallel_results' in locals() and parallel_results:\n",
    "    par_df = pd.DataFrame.from_dict(parallel_results, orient='index')\n",
    "    max_speedup = par_df['speedup'].max() if 'speedup' in par_df.columns else 0\n",
    "    max_efficiency = par_df['efficiency'].max() if 'efficiency' in par_df.columns else 0\n",
    "    \n",
    "    conclusions.append(\"\\n并行DBSCAN性能:\")\n",
    "    conclusions.append(f\"  • 最大加速比: {max_speedup:.2f}x\")\n",
    "    conclusions.append(f\"  • 最大并行效率: {max_efficiency:.1%}\")\n",
    "    conclusions.append(f\"  • 最优工作进程数: {par_df.loc[par_df['speedup'].idxmax(), 'n_jobs']}\")\n",
    "\n",
    "# 综合比较结论\n",
    "if 'comparison_results' in locals() and not comparison_results.empty:\n",
    "    fastest_impl = comparison_results.loc[comparison_results['execution_time'].idxmin()]\n",
    "    slowest_impl = comparison_results.loc[comparison_results['execution_time'].idxmax()]\n",
    "    \n",
    "    speedup_ratio = slowest_impl['execution_time'] / fastest_impl['execution_time']\n",
    "    \n",
    "    conclusions.append(\"\\n算法实现比较:\")\n",
    "    conclusions.append(f\"  • 最快实现: {fastest_impl['implementation']}\")\n",
    "    conclusions.append(f\"  • 最慢实现: {slowest_impl['implementation']}\")\n",
    "    conclusions.append(f\"  • 性能差异: {speedup_ratio:.2f}x\")\n",
    "\n",
    "# 瓶颈分析结论\n",
    "if 'time_analysis' in locals() and 'memory_profiler' in locals():\n",
    "    conclusions.append(\"\\n主要性能瓶颈:\")\n",
    "    \n",
    "    if 'top_functions' in time_analysis:\n",
    "        main_bottleneck = time_analysis['top_functions'][0]\n",
    "        time_percent = (main_bottleneck['cumtime'] / time_analysis['execution_time']) * 100\n",
    "        conclusions.append(f\"  • 时间瓶颈: {main_bottleneck['function']} ({time_percent:.1f}%)\")\n",
    "    \n",
    "    bottlenecks = memory_profiler.find_memory_bottlenecks(threshold_mb=5.0)\n",
    "    if bottlenecks:\n",
    "        main_memory_bottleneck = bottlenecks[0]\n",
    "        conclusions.append(f\"  • 内存瓶颈: {main_memory_bottleneck['consumer']} ({main_memory_bottleneck['size_mb']:.2f} MB)\")\n",
    "\n",
    "# 优化建议总结\n",
    "conclusions.append(\"\\n关键优化建议:\")\n",
    "conclusions.append(\"  1. 使用空间索引加速邻居搜索\")\n",
    "conclusions.append(\"  2. 利用多核CPU进行并行计算\")\n",
    "conclusions.append(f\"  3. 建议使用 {max(1, mp.cpu_count() - 2)} 个工作进程\")\n",
    "conclusions.append(\"  4. 使用向量化操作代替Python循环\")\n",
    "conclusions.append(\"  5. 及时释放大对象，优化内存使用\")\n",
    "\n",
    "# 打印所有结论\n",
    "for conclusion in conclusions:\n",
    "    print(conclusion)\n",
    "\n",
    "# 保存分析结果\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"结果保存\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_dir = \"../results/notebooks\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 保存串行结果\n",
    "if 'sequential_results' in locals():\n",
    "    seq_df.to_csv(f\"{results_dir}/sequential_results_{timestamp}.csv\")\n",
    "\n",
    "# 保存并行结果\n",
    "if 'parallel_results' in locals():\n",
    "    par_df.to_csv(f\"{results_dir}/parallel_results_{timestamp}.csv\")\n",
    "\n",
    "# 保存比较结果\n",
    "if 'comparison_results' in locals():\n",
    "    comparison_results.to_csv(f\"{results_dir}/comparison_results_{timestamp}.csv\")\n",
    "\n",
    "# 保存结论\n",
    "with open(f\"{results_dir}/conclusions_{timestamp}.txt\", \"w\") as f:\n",
    "    for conclusion in conclusions:\n",
    "        f.write(conclusion + \"\\n\")\n",
    "\n",
    "print(f\"分析结果已保存到: {results_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "876de36874448f55",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
